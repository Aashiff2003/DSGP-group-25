{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "drive_path='/content/drive/Mydrive/'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FF7ynoDrmV37",
        "outputId": "594d2290-5800-4eff-aa3b-daa016fb3504"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Define the dataset path\n",
        "base_dir = '/content/drive/MyDrive/dataset'\n",
        "# Define parameters for image processing\n",
        "img_height, img_width = 224, 224  # Image size for input\n",
        "batch_size = 32\n",
        "\n",
        "# Use ImageDataGenerator to load and augment images\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1.0/255,          # Normalize pixel values to [0, 1]\n",
        "    rotation_range=30,        # Randomly rotate images\n",
        "    width_shift_range=0.2,    # Shift images horizontally\n",
        "    height_shift_range=0.2,   # Shift images vertically\n",
        "    shear_range=0.2,          # Apply shear transformation\n",
        "    zoom_range=0.2,           # Apply zoom\n",
        "    horizontal_flip=True,     # Flip images horizontally\n",
        "    validation_split=0.2      # Use 20% of the data for validation\n",
        ")\n",
        "\n",
        "# Create training and validation generators\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    base_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='training'  # Use training subset\n",
        ")\n",
        "\n",
        "val_generator = train_datagen.flow_from_directory(\n",
        "    base_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='validation'  # Use validation subset\n",
        ")\n",
        "\n",
        "# Verify class indices\n",
        "print(\"Class Indices:\", train_generator.class_indices)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mui4i-nxnYyl",
        "outputId": "bd8258d6-4c9d-4c65-ebdc-eeee02986305"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1224 images belonging to 6 classes.\n",
            "Found 306 images belonging to 6 classes.\n",
            "Class Indices: {'alien_test': 0, 'cloudy': 1, 'foggy': 2, 'rainy': 3, 'shine': 4, 'sunrise': 5}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from collections import Counter\n",
        "\n",
        "# Define the dataset path\n",
        "base_dir = '/content/drive/MyDrive/dataset'\n",
        "\n",
        "# Define parameters for image processing\n",
        "img_height, img_width = 224, 224  # Image size for input\n",
        "batch_size = 32\n",
        "\n",
        "# Data Augmentation and Preprocessing\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1.0/255,          # Normalize pixel values to [0, 1]\n",
        "    rotation_range=30,        # Randomly rotate images\n",
        "    width_shift_range=0.2,    # Shift images horizontally\n",
        "    height_shift_range=0.2,   # Shift images vertically\n",
        "    shear_range=0.2,          # Apply shear transformation\n",
        "    zoom_range=0.2,           # Apply zoom\n",
        "    horizontal_flip=True,     # Flip images horizontally\n",
        "    validation_split=0.2      # Use 20% of the data for validation\n",
        ")\n",
        "\n",
        "# Training and Validation Generators\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    base_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='training'  # Use training subset\n",
        ")\n",
        "\n",
        "val_generator = train_datagen.flow_from_directory(\n",
        "    base_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='validation'  # Use validation subset\n",
        ")\n",
        "\n",
        "# Check class indices\n",
        "print(\"Class Indices:\", train_generator.class_indices)\n",
        "\n",
        "# Define the CNN Model\n",
        "model = Sequential([\n",
        "    Input(shape=(img_height, img_width, 3)),\n",
        "    Conv2D(32, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(len(train_generator.class_indices), activation='softmax')  # Adjust output layer for the number of classes\n",
        "])\n",
        "\n",
        "# Compile the Model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Define Callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "model_checkpoint = ModelCheckpoint('best_weather_model.keras', save_best_only=True, monitor='val_loss', mode='min')\n",
        "\n",
        "# Train the Model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=50,\n",
        "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
        "    validation_steps=val_generator.samples // val_generator.batch_size,\n",
        "    callbacks=[early_stopping, model_checkpoint]\n",
        ")\n",
        "\n",
        "# Save the final model\n",
        "model.save('final_weather_classification_model.keras')\n",
        "\n",
        "# Evaluate the Model\n",
        "loss, accuracy = model.evaluate(val_generator)\n",
        "print(f\"Validation Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Make Predictions\n",
        "import numpy as np\n",
        "\n",
        "# Predict on validation set (example)\n",
        "sample_images, sample_labels = next(val_generator)\n",
        "predictions = model.predict(sample_images)\n",
        "\n",
        "# Decode Predictions\n",
        "predicted_classes = np.argmax(predictions, axis=1)\n",
        "actual_classes = np.argmax(sample_labels, axis=1)\n",
        "\n",
        "print(f\"Predicted Classes: {predicted_classes}\")\n",
        "print(f\"Actual Classes: {actual_classes}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VSVhY3tvWy6",
        "outputId": "af4d1c51-7e91-45b3-ea50-5b3881abc843"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1224 images belonging to 6 classes.\n",
            "Found 306 images belonging to 6 classes.\n",
            "Class Indices: {'alien_test': 0, 'cloudy': 1, 'foggy': 2, 'rainy': 3, 'shine': 4, 'sunrise': 5}\n",
            "Epoch 1/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m221s\u001b[0m 6s/step - accuracy: 0.3913 - loss: 1.6969 - val_accuracy: 0.6076 - val_loss: 1.3149\n",
            "Epoch 2/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 39ms/step - accuracy: 0.6250 - loss: 1.4739 - val_accuracy: 0.6111 - val_loss: 1.2011\n",
            "Epoch 3/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m216s\u001b[0m 5s/step - accuracy: 0.5707 - loss: 1.1263 - val_accuracy: 0.6146 - val_loss: 1.1347\n",
            "Epoch 4/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 40ms/step - accuracy: 0.5312 - loss: 1.1415 - val_accuracy: 0.5556 - val_loss: 1.0136\n",
            "Epoch 5/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m269s\u001b[0m 6s/step - accuracy: 0.6457 - loss: 0.9185 - val_accuracy: 0.6806 - val_loss: 0.8856\n",
            "Epoch 6/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - accuracy: 0.7188 - loss: 0.9111 - val_accuracy: 0.5556 - val_loss: 1.0751\n",
            "Epoch 7/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m241s\u001b[0m 5s/step - accuracy: 0.6330 - loss: 0.9743 - val_accuracy: 0.6944 - val_loss: 0.9007\n",
            "Epoch 8/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - accuracy: 0.7188 - loss: 0.8128 - val_accuracy: 0.6667 - val_loss: 0.8876\n",
            "Epoch 9/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m228s\u001b[0m 6s/step - accuracy: 0.6795 - loss: 0.8722 - val_accuracy: 0.7049 - val_loss: 0.8538\n",
            "Epoch 10/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 324ms/step - accuracy: 0.7500 - loss: 1.0132 - val_accuracy: 0.6667 - val_loss: 0.6488\n",
            "Epoch 11/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m238s\u001b[0m 6s/step - accuracy: 0.7216 - loss: 0.8267 - val_accuracy: 0.6667 - val_loss: 0.9569\n",
            "Epoch 12/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 36ms/step - accuracy: 0.6562 - loss: 1.1635 - val_accuracy: 0.7222 - val_loss: 0.6450\n",
            "Epoch 13/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m259s\u001b[0m 6s/step - accuracy: 0.7644 - loss: 0.7451 - val_accuracy: 0.7326 - val_loss: 0.8131\n",
            "Epoch 14/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - accuracy: 0.6875 - loss: 0.7938 - val_accuracy: 0.6667 - val_loss: 0.7851\n",
            "Epoch 15/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m220s\u001b[0m 6s/step - accuracy: 0.7317 - loss: 0.7852 - val_accuracy: 0.6840 - val_loss: 0.8887\n",
            "Epoch 16/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 538ms/step - accuracy: 0.6875 - loss: 1.0545 - val_accuracy: 0.7222 - val_loss: 0.5940\n",
            "Epoch 17/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m228s\u001b[0m 5s/step - accuracy: 0.7600 - loss: 0.7761 - val_accuracy: 0.6979 - val_loss: 0.8598\n",
            "Epoch 18/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 17ms/step - accuracy: 0.8438 - loss: 0.5724 - val_accuracy: 0.4444 - val_loss: 1.6403\n",
            "Epoch 19/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m266s\u001b[0m 6s/step - accuracy: 0.7625 - loss: 0.7110 - val_accuracy: 0.7292 - val_loss: 0.7768\n",
            "Epoch 20/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.6875 - loss: 0.8580 - val_accuracy: 0.8333 - val_loss: 0.6989\n",
            "Epoch 21/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m253s\u001b[0m 6s/step - accuracy: 0.7774 - loss: 0.6798 - val_accuracy: 0.7083 - val_loss: 0.8214\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 2s/step - accuracy: 0.6745 - loss: 0.8938\n",
            "Validation Accuracy: 68.30%\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
            "Predicted Classes: [2 3 4 4 1 5 3 4 4 4 1 2 3 2 1 2 1 5 4 1 5 5 3 4 3 3 1 1 3 5 4 1]\n",
            "Actual Classes: [4 3 1 1 1 5 3 4 4 1 2 2 3 2 2 2 2 5 4 2 5 5 4 4 3 3 3 1 3 5 4 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from collections import Counter\n",
        "\n",
        "# Define the dataset path\n",
        "base_dir = '/content/drive/MyDrive/dataset'\n",
        "\n",
        "# Define parameters for image processing\n",
        "img_height, img_width = 224, 224\n",
        "batch_size = 32\n",
        "\n",
        "# Data Augmentation and Preprocessing\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1.0 / 255,\n",
        "    rotation_range=30,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    brightness_range=[0.5, 1.5],  # Adjust brightness\n",
        "    horizontal_flip=True,\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    base_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "val_generator = train_datagen.flow_from_directory(\n",
        "    base_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='validation'\n",
        ")\n",
        "\n",
        "# Check class distribution and calculate class weights\n",
        "class_counts = Counter(train_generator.classes)\n",
        "class_weights = {i: max(class_counts.values()) / count for i, count in class_counts.items()}\n",
        "print(\"Class Weights:\", class_weights)\n",
        "\n",
        "# Define an Improved CNN Model\n",
        "model = Sequential([\n",
        "    Input(shape=(img_height, img_width, 3)),\n",
        "    Conv2D(32, (3, 3), activation='relu'),\n",
        "    BatchNormalization(),  # Normalize activations\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Conv2D(256, (3, 3), activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dropout(0.5),  # Reduce overfitting\n",
        "    Dense(len(train_generator.class_indices), activation='softmax')  # Output layer\n",
        "])\n",
        "\n",
        "# Compile the Model with Learning Rate Scheduler\n",
        "model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Define Callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "model_checkpoint = ModelCheckpoint('best_weather_model_improved.keras', save_best_only=True, monitor='val_loss', mode='min')\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
        "\n",
        "# Train the Model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=50,\n",
        "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
        "    validation_steps=val_generator.samples // val_generator.batch_size,\n",
        "    class_weight=class_weights,  # Handle imbalanced classes\n",
        "    callbacks=[early_stopping, model_checkpoint, reduce_lr]\n",
        ")\n",
        "\n",
        "# Save the final model\n",
        "model.save('final_weather_classification_model_improved.keras')\n",
        "\n",
        "# Evaluate the Model\n",
        "loss, accuracy = model.evaluate(val_generator)\n",
        "print(f\"Validation Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Make Predictions\n",
        "import numpy as np\n",
        "\n",
        "# Predict on validation set (example)\n",
        "sample_images, sample_labels = next(val_generator)\n",
        "predictions = model.predict(sample_images)\n",
        "\n",
        "# Decode Predictions\n",
        "predicted_classes = np.argmax(predictions, axis=1)\n",
        "actual_classes = np.argmax(sample_labels, axis=1)\n",
        "\n",
        "print(f\"Predicted Classes: {predicted_classes}\")\n",
        "print(f\"Actual Classes: {actual_classes}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvaOGbNdLopl",
        "outputId": "b2c4c5ea-fd89-4ccf-fe92-583867165fdc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1224 images belonging to 6 classes.\n",
            "Found 306 images belonging to 6 classes.\n",
            "Class Weights: {0: 11.666666666666666, 1: 1.1666666666666667, 2: 1.1666666666666667, 3: 1.1666666666666667, 4: 1.4, 5: 1.0}\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m616s\u001b[0m 14s/step - accuracy: 0.4448 - loss: 24.0305 - val_accuracy: 0.1944 - val_loss: 11.7483 - learning_rate: 0.0010\n",
            "Epoch 2/50\n",
            "\u001b[1m 1/38\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:47\u001b[0m 6s/step - accuracy: 0.5625 - loss: 11.8863"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.11/contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self.gen.throw(typ, value, traceback)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 38ms/step - accuracy: 0.5625 - loss: 11.8863 - val_accuracy: 0.3333 - val_loss: 8.6557 - learning_rate: 0.0010\n",
            "Epoch 3/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m285s\u001b[0m 7s/step - accuracy: 0.4754 - loss: 32.3340 - val_accuracy: 0.2153 - val_loss: 14.3871 - learning_rate: 0.0010\n",
            "Epoch 4/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 17ms/step - accuracy: 0.3438 - loss: 22.5178 - val_accuracy: 0.3889 - val_loss: 14.5146 - learning_rate: 0.0010\n",
            "Epoch 5/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m270s\u001b[0m 7s/step - accuracy: 0.4563 - loss: 37.4952 - val_accuracy: 0.2222 - val_loss: 21.7259 - learning_rate: 0.0010\n",
            "Epoch 6/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 449ms/step - accuracy: 0.3750 - loss: 21.5459 - val_accuracy: 0.1667 - val_loss: 25.3533 - learning_rate: 5.0000e-04\n",
            "Epoch 7/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m307s\u001b[0m 7s/step - accuracy: 0.4828 - loss: 29.4962 - val_accuracy: 0.2431 - val_loss: 9.6098 - learning_rate: 5.0000e-04\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 2s/step - accuracy: 0.2291 - loss: 10.7783\n",
            "Validation Accuracy: 20.59%\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
            "Predicted Classes: [1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            "Actual Classes: [4 1 4 1 1 2 2 5 3 5 1 3 3 2 3 3 5 4 5 1 2 3 3 1 3 2 1 1 1 4 4 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from collections import Counter\n",
        "\n",
        "# Define the dataset path\n",
        "base_dir = '/content/drive/MyDrive/dataset'\n",
        "\n",
        "# Define parameters for image processing\n",
        "img_height, img_width = 224, 224\n",
        "batch_size = 32\n",
        "\n",
        "# Data Augmentation and Preprocessing\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1.0 / 255,\n",
        "    rotation_range=30,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    brightness_range=[0.5, 1.5],  # Added brightness adjustment\n",
        "    horizontal_flip=True,\n",
        "    validation_split=0.2  # 20% for validation\n",
        ")\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    base_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "val_generator = train_datagen.flow_from_directory(\n",
        "    base_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='validation'\n",
        ")\n",
        "\n",
        "# Check class distribution and calculate class weights\n",
        "class_counts = Counter(train_generator.classes)\n",
        "class_weights = {i: max(class_counts.values()) / count for i, count in class_counts.items()}\n",
        "print(\"Class Weights:\", class_weights)\n",
        "\n",
        "# Load the ResNet50 model for transfer learning\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(img_height, img_width, 3))\n",
        "base_model.trainable = False  # Freeze the base model layers\n",
        "\n",
        "# Build the model\n",
        "model = Sequential([\n",
        "    base_model,\n",
        "    Flatten(),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(len(train_generator.class_indices), activation='softmax')  # Output layer\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Define callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "model_checkpoint = ModelCheckpoint('best_weather_model_resnet.keras', save_best_only=True, monitor='val_loss', mode='min')\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=50,\n",
        "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
        "    validation_steps=val_generator.samples // val_generator.batch_size,\n",
        "    class_weight=class_weights,  # Apply class weights\n",
        "    callbacks=[early_stopping, model_checkpoint, reduce_lr]\n",
        ")\n",
        "\n",
        "# Save the final model\n",
        "model.save('final_weather_classification_model_resnet.keras')\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(val_generator)\n",
        "print(f\"Validation Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Make Predictions (example on validation data)\n",
        "import numpy as np\n",
        "\n",
        "sample_images, sample_labels = next(val_generator)\n",
        "predictions = model.predict(sample_images)\n",
        "\n",
        "# Decode Predictions\n",
        "predicted_classes = np.argmax(predictions, axis=1)\n",
        "actual_classes = np.argmax(sample_labels, axis=1)\n",
        "\n",
        "print(f\"Predicted Classes: {predicted_classes}\")\n",
        "print(f\"Actual Classes: {actual_classes}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8G-FKlYMEjOx",
        "outputId": "af50467d-3127-4271-f1c1-7ceae28e3787"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1224 images belonging to 6 classes.\n",
            "Found 306 images belonging to 6 classes.\n",
            "Class Weights: {0: 11.666666666666666, 1: 1.1666666666666667, 2: 1.1666666666666667, 3: 1.1666666666666667, 4: 1.4, 5: 1.0}\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m749s\u001b[0m 17s/step - accuracy: 0.2053 - loss: 10.0087 - val_accuracy: 0.1910 - val_loss: 1.8533 - learning_rate: 0.0010\n",
            "Epoch 2/50\n",
            "\u001b[1m 1/38\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:37\u001b[0m 6s/step - accuracy: 0.3125 - loss: 2.1871"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.11/contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self.gen.throw(typ, value, traceback)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 484ms/step - accuracy: 0.3125 - loss: 2.1871 - val_accuracy: 0.1667 - val_loss: 1.7921 - learning_rate: 0.0010\n",
            "Epoch 3/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m362s\u001b[0m 9s/step - accuracy: 0.2027 - loss: 2.4666 - val_accuracy: 0.0174 - val_loss: 1.7924 - learning_rate: 0.0010\n",
            "Epoch 4/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 166ms/step - accuracy: 0.0000e+00 - loss: 2.1107 - val_accuracy: 0.0556 - val_loss: 1.7919 - learning_rate: 0.0010\n",
            "Epoch 5/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m371s\u001b[0m 10s/step - accuracy: 0.0263 - loss: 2.5597 - val_accuracy: 0.0208 - val_loss: 1.7938 - learning_rate: 0.0010\n",
            "Epoch 6/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 84ms/step - accuracy: 0.0312 - loss: 2.6419 - val_accuracy: 0.0000e+00 - val_loss: 1.7941 - learning_rate: 0.0010\n",
            "Epoch 7/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m357s\u001b[0m 9s/step - accuracy: 0.0228 - loss: 2.5255 - val_accuracy: 0.0208 - val_loss: 1.7936 - learning_rate: 0.0010\n",
            "Epoch 8/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 81ms/step - accuracy: 0.0000e+00 - loss: 2.0607 - val_accuracy: 0.0000e+00 - val_loss: 1.7937 - learning_rate: 5.0000e-04\n",
            "Epoch 9/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m363s\u001b[0m 9s/step - accuracy: 0.0251 - loss: 2.5276 - val_accuracy: 0.0208 - val_loss: 1.7937 - learning_rate: 5.0000e-04\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 7s/step - accuracy: 0.0083 - loss: 1.7927\n",
            "Validation Accuracy: 1.96%\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8s/step\n",
            "Predicted Classes: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Actual Classes: [3 5 4 2 5 5 3 3 3 5 5 5 2 5 4 5 1 1 3 2 5 4 2 2 3 4 4 2 3 2 1 5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "# Define the dataset path\n",
        "base_dir = '/content/drive/MyDrive/dataset'\n",
        "\n",
        "# Define parameters for image processing\n",
        "img_height, img_width = 224, 224\n",
        "batch_size = 32\n",
        "\n",
        "# Data Augmentation and Preprocessing\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1.0 / 255,\n",
        "    rotation_range=30,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    brightness_range=[0.5, 1.5],  # Adjust brightness\n",
        "    horizontal_flip=True,\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    base_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "val_generator = train_datagen.flow_from_directory(\n",
        "    base_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='validation'\n",
        ")\n",
        "\n",
        "# Check class distribution and calculate class weights\n",
        "class_counts = Counter(train_generator.classes)\n",
        "class_weights = {i: max(class_counts.values()) / count for i, count in class_counts.items()}\n",
        "print(\"Class Weights:\", class_weights)\n",
        "\n",
        "# Define a CNN Model with Batch Normalization\n",
        "model = Sequential([\n",
        "    Input(shape=(img_height, img_width, 3)),\n",
        "    Conv2D(32, (3, 3), activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Conv2D(256, (3, 3), activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dropout(0.5),  # Dropout to reduce overfitting\n",
        "    Dense(len(train_generator.class_indices), activation='softmax')  # Output layer\n",
        "])\n",
        "\n",
        "# Compile the Model with Adam Optimizer and Learning Rate Scheduler\n",
        "model.compile(optimizer=Adam(learning_rate=1e-4),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Define Callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "model_checkpoint = ModelCheckpoint('best_weather_model.keras', save_best_only=True, monitor='val_loss', mode='min')\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
        "\n",
        "# Train the Model\n",
        "steps_per_epoch = train_generator.samples // train_generator.batch_size\n",
        "validation_steps = val_generator.samples // val_generator.batch_size\n",
        "\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=50,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    validation_steps=validation_steps,\n",
        "    class_weight=class_weights,\n",
        "    callbacks=[early_stopping, model_checkpoint, reduce_lr]\n",
        ")\n",
        "\n",
        "# Save the Final Model\n",
        "model.save('final_weather_classification_model.keras')\n",
        "\n",
        "# Evaluate the Model\n",
        "loss, accuracy = model.evaluate(val_generator)\n",
        "print(f\"Validation Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Predict on Validation Data\n",
        "val_predictions = model.predict(val_generator)\n",
        "y_pred = np.argmax(val_predictions, axis=1)\n",
        "y_true = val_generator.classes\n",
        "\n",
        "# Confusion Matrix and Classification Report\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_true, y_pred))\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_true, y_pred, target_names=list(train_generator.class_indices.keys())))\n",
        "\n",
        "# Predict on Sample Batch\n",
        "sample_images, sample_labels = next(val_generator)\n",
        "predictions = model.predict(sample_images)\n",
        "\n",
        "# Decode Predictions\n",
        "predicted_classes = np.argmax(predictions, axis=1)\n",
        "actual_classes = np.argmax(sample_labels, axis=1)\n",
        "\n",
        "print(f\"Predicted Classes: {predicted_classes}\")\n",
        "print(f\"Actual Classes: {actual_classes}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDhGFbC6LGv0",
        "outputId": "4e6fba1c-ac51-44e9-f355-8f159f3e35eb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1224 images belonging to 6 classes.\n",
            "Found 306 images belonging to 6 classes.\n",
            "Class Weights: {0: 11.666666666666666, 1: 1.1666666666666667, 2: 1.1666666666666667, 3: 1.1666666666666667, 4: 1.4, 5: 1.0}\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m671s\u001b[0m 15s/step - accuracy: 0.4606 - loss: 5.8965 - val_accuracy: 0.2257 - val_loss: 1.9048 - learning_rate: 1.0000e-04\n",
            "Epoch 2/50\n",
            "\u001b[1m 1/38\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5:23\u001b[0m 9s/step - accuracy: 0.4375 - loss: 2.1182"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.11/contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self.gen.throw(typ, value, traceback)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.4375 - loss: 2.1182 - val_accuracy: 0.0556 - val_loss: 2.3495 - learning_rate: 1.0000e-04\n",
            "Epoch 3/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m316s\u001b[0m 8s/step - accuracy: 0.5328 - loss: 4.3304 - val_accuracy: 0.1910 - val_loss: 4.3268 - learning_rate: 1.0000e-04\n",
            "Epoch 4/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.3438 - loss: 2.9720 - val_accuracy: 0.2778 - val_loss: 3.8887 - learning_rate: 1.0000e-04\n",
            "Epoch 5/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m369s\u001b[0m 8s/step - accuracy: 0.5854 - loss: 3.3024 - val_accuracy: 0.1840 - val_loss: 4.4857 - learning_rate: 5.0000e-05\n",
            "Epoch 6/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.5938 - loss: 2.4088 - val_accuracy: 0.3889 - val_loss: 3.5976 - learning_rate: 5.0000e-05\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 2s/step - accuracy: 0.2232 - loss: 1.9240\n",
            "Validation Accuracy: 21.57%\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2s/step\n",
            "\n",
            "Confusion Matrix:\n",
            "[[ 0  2  4  0  0  0]\n",
            " [ 0  5 55  0  0  0]\n",
            " [ 0  6 54  0  0  0]\n",
            " [ 0  3 57  0  0  0]\n",
            " [ 0  1 49  0  0  0]\n",
            " [ 0  8 62  0  0  0]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "  alien_test       0.00      0.00      0.00         6\n",
            "      cloudy       0.20      0.08      0.12        60\n",
            "       foggy       0.19      0.90      0.32        60\n",
            "       rainy       0.00      0.00      0.00        60\n",
            "       shine       0.00      0.00      0.00        50\n",
            "     sunrise       0.00      0.00      0.00        70\n",
            "\n",
            "    accuracy                           0.19       306\n",
            "   macro avg       0.07      0.16      0.07       306\n",
            "weighted avg       0.08      0.19      0.09       306\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
            "Predicted Classes: [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2]\n",
            "Actual Classes: [1 4 2 5 5 5 2 1 3 1 0 1 5 1 4 5 4 2 1 2 2 4 2 1 2 2 4 5 1 5 4 5]\n"
          ]
        }
      ]
    }
  ]
}