{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "drive_path='/content/drive/Mydrive/'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FF7ynoDrmV37",
        "outputId": "bdb3f1f3-1e00-4521-a737-2c96ab1ed30e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Define paths\n",
        "base_dir = '/content/drive/MyDrive/dataset'\n",
        "csv_path = os.path.join(base_dir, 'test.csv')  # Adjust if your CSV file is named differently\n",
        "image_dir = base_dir  # Images are inside the subfolders of `dataset`\n",
        "\n",
        "# Load CSV file\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Map image paths to full paths\n",
        "df['image_path'] = df['Image_id'].apply(lambda x: os.path.join(image_dir, x))\n",
        "df = df[df['image_path'].apply(lambda x: os.path.exists(x))]  # Filter rows where images exist\n",
        "\n",
        "# Check data\n",
        "print(f\"Number of images in dataset: {len(df)}\")\n",
        "print(df.head())\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_df, val_df = train_test_split(df, test_size=0.2, stratify=df['labels'], random_state=42)\n",
        "\n",
        "# Define parameters for image processing\n",
        "img_height, img_width = 224, 224\n",
        "batch_size = 32\n",
        "\n",
        "# Use ImageDataGenerator to load and augment images\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1.0/255,\n",
        "    rotation_range=30,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale=1.0/255)\n",
        "\n",
        "# Create training and validation generators\n",
        "train_generator = train_datagen.flow_from_dataframe(\n",
        "    train_df,\n",
        "    x_col='image_path',\n",
        "    y_col='labels',\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "val_generator = val_datagen.flow_from_dataframe(\n",
        "    val_df,\n",
        "    x_col='image_path',\n",
        "    y_col='labels',\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# Verify the class indices\n",
        "print(\"Class Indices:\", train_generator.class_indices)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "aO3gR_PGmamH",
        "outputId": "2e5e5e67-f45c-443b-cfa9-69388aa0ed19"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of images in dataset: 0\n",
            "Empty DataFrame\n",
            "Columns: [Image_id, labels, image_path]\n",
            "Index: []\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-a5d3500ba67a>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Split the dataset into training and validation sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstratify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Define parameters for image processing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2850\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2851\u001b[0;31m     n_train, n_test = _validate_shuffle_split(\n\u001b[0m\u001b[1;32m   2852\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_test_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2853\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   2479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn_train\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2481\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   2482\u001b[0m             \u001b[0;34m\"With n_samples={}, test_size={} and train_size={}, the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2483\u001b[0m             \u001b[0;34m\"resulting train set will be empty. Adjust any of the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Define the dataset path\n",
        "base_dir = '/content/drive/MyDrive/dataset'  # Replace with the actual dataset folder path\n",
        "\n",
        "# Define parameters for image processing\n",
        "img_height, img_width = 224, 224  # Image size for input\n",
        "batch_size = 32\n",
        "\n",
        "# Use ImageDataGenerator to load and augment images\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1.0/255,          # Normalize pixel values to [0, 1]\n",
        "    rotation_range=30,        # Randomly rotate images\n",
        "    width_shift_range=0.2,    # Shift images horizontally\n",
        "    height_shift_range=0.2,   # Shift images vertically\n",
        "    shear_range=0.2,          # Apply shear transformation\n",
        "    zoom_range=0.2,           # Apply zoom\n",
        "    horizontal_flip=True,     # Flip images horizontally\n",
        "    validation_split=0.2      # Use 20% of the data for validation\n",
        ")\n",
        "\n",
        "# Create training and validation generators\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    base_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='training'  # Use training subset\n",
        ")\n",
        "\n",
        "val_generator = train_datagen.flow_from_directory(\n",
        "    base_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='validation'  # Use validation subset\n",
        ")\n",
        "\n",
        "# Verify class indices\n",
        "print(\"Class Indices:\", train_generator.class_indices)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mui4i-nxnYyl",
        "outputId": "a1c0d01e-9739-42cc-9d57-1d7bbdf1989d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1224 images belonging to 6 classes.\n",
            "Found 306 images belonging to 6 classes.\n",
            "Class Indices: {'alien_test': 0, 'cloudy': 1, 'foggy': 2, 'rainy': 3, 'shine': 4, 'sunrise': 5}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Data Augmentation and Generators\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=30,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    'path_to_train_dataset',     # Replace with your training dataset path\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "val_generator = val_datagen.flow_from_directory(\n",
        "    'path_to_val_dataset',       # Replace with your validation dataset path\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# Model Definition\n",
        "model = Sequential([\n",
        "    Input(shape=(224, 224, 3)),\n",
        "    Conv2D(32, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(6, activation='softmax')  # 6 classes\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Early Stopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Calculate steps per epoch\n",
        "steps_per_epoch = train_generator.samples // train_generator.batch_size\n",
        "validation_steps = val_generator.samples // val_generator.batch_size\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=50,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    validation_steps=validation_steps,\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "# Save the model\n",
        "model.save('weather_classification_model.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "nPRPiOXTngmA",
        "outputId": "2244c04f-0d39-4c1b-bb59-6030733a0810"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'path_to_train_dataset'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-276b6bb67ed8>\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mval_datagen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageDataGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrescale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m train_generator = train_datagen.flow_from_directory(\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0;34m'path_to_train_dataset'\u001b[0m\u001b[0;34m,\u001b[0m     \u001b[0;31m# Replace with your training dataset path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/legacy/preprocessing/image.py\u001b[0m in \u001b[0;36mflow_from_directory\u001b[0;34m(self, directory, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[1;32m   1136\u001b[0m         \u001b[0mkeep_aspect_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m     ):\n\u001b[0;32m-> 1138\u001b[0;31m         return DirectoryIterator(\n\u001b[0m\u001b[1;32m   1139\u001b[0m             \u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/legacy/preprocessing/image.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio, dtype)\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m             \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0msubdir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                     \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'path_to_train_dataset'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "model.save('weather_classification_model.keras')"
      ],
      "metadata": {
        "id": "4BJNfoPiv_GS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from collections import Counter\n",
        "\n",
        "# Define the dataset path\n",
        "base_dir = '/content/drive/MyDrive/dataset'  # Replace with your dataset folder path\n",
        "\n",
        "# Define parameters for image processing\n",
        "img_height, img_width = 224, 224  # Image size for input\n",
        "batch_size = 32\n",
        "\n",
        "# Data Augmentation and Preprocessing\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1.0/255,          # Normalize pixel values to [0, 1]\n",
        "    rotation_range=30,        # Randomly rotate images\n",
        "    width_shift_range=0.2,    # Shift images horizontally\n",
        "    height_shift_range=0.2,   # Shift images vertically\n",
        "    shear_range=0.2,          # Apply shear transformation\n",
        "    zoom_range=0.2,           # Apply zoom\n",
        "    horizontal_flip=True,     # Flip images horizontally\n",
        "    validation_split=0.2      # Use 20% of the data for validation\n",
        ")\n",
        "\n",
        "# Training and Validation Generators\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    base_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='training'  # Use training subset\n",
        ")\n",
        "\n",
        "val_generator = train_datagen.flow_from_directory(\n",
        "    base_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='validation'  # Use validation subset\n",
        ")\n",
        "\n",
        "# Check class indices\n",
        "print(\"Class Indices:\", train_generator.class_indices)\n",
        "\n",
        "# Define the CNN Model\n",
        "model = Sequential([\n",
        "    Input(shape=(img_height, img_width, 3)),\n",
        "    Conv2D(32, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(len(train_generator.class_indices), activation='softmax')  # Adjust output layer for the number of classes\n",
        "])\n",
        "\n",
        "# Compile the Model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Define Callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "model_checkpoint = ModelCheckpoint('best_weather_model.keras', save_best_only=True, monitor='val_loss', mode='min')\n",
        "\n",
        "# Train the Model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=50,\n",
        "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
        "    validation_steps=val_generator.samples // val_generator.batch_size,\n",
        "    callbacks=[early_stopping, model_checkpoint]\n",
        ")\n",
        "\n",
        "# Save the final model\n",
        "model.save('final_weather_classification_model.keras')\n",
        "\n",
        "# Evaluate the Model\n",
        "loss, accuracy = model.evaluate(val_generator)\n",
        "print(f\"Validation Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Make Predictions\n",
        "import numpy as np\n",
        "\n",
        "# Predict on validation set (example)\n",
        "sample_images, sample_labels = next(val_generator)\n",
        "predictions = model.predict(sample_images)\n",
        "\n",
        "# Decode Predictions\n",
        "predicted_classes = np.argmax(predictions, axis=1)\n",
        "actual_classes = np.argmax(sample_labels, axis=1)\n",
        "\n",
        "print(f\"Predicted Classes: {predicted_classes}\")\n",
        "print(f\"Actual Classes: {actual_classes}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VSVhY3tvWy6",
        "outputId": "af4d1c51-7e91-45b3-ea50-5b3881abc843"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1224 images belonging to 6 classes.\n",
            "Found 306 images belonging to 6 classes.\n",
            "Class Indices: {'alien_test': 0, 'cloudy': 1, 'foggy': 2, 'rainy': 3, 'shine': 4, 'sunrise': 5}\n",
            "Epoch 1/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m221s\u001b[0m 6s/step - accuracy: 0.3913 - loss: 1.6969 - val_accuracy: 0.6076 - val_loss: 1.3149\n",
            "Epoch 2/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 39ms/step - accuracy: 0.6250 - loss: 1.4739 - val_accuracy: 0.6111 - val_loss: 1.2011\n",
            "Epoch 3/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m216s\u001b[0m 5s/step - accuracy: 0.5707 - loss: 1.1263 - val_accuracy: 0.6146 - val_loss: 1.1347\n",
            "Epoch 4/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 40ms/step - accuracy: 0.5312 - loss: 1.1415 - val_accuracy: 0.5556 - val_loss: 1.0136\n",
            "Epoch 5/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m269s\u001b[0m 6s/step - accuracy: 0.6457 - loss: 0.9185 - val_accuracy: 0.6806 - val_loss: 0.8856\n",
            "Epoch 6/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - accuracy: 0.7188 - loss: 0.9111 - val_accuracy: 0.5556 - val_loss: 1.0751\n",
            "Epoch 7/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m241s\u001b[0m 5s/step - accuracy: 0.6330 - loss: 0.9743 - val_accuracy: 0.6944 - val_loss: 0.9007\n",
            "Epoch 8/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - accuracy: 0.7188 - loss: 0.8128 - val_accuracy: 0.6667 - val_loss: 0.8876\n",
            "Epoch 9/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m228s\u001b[0m 6s/step - accuracy: 0.6795 - loss: 0.8722 - val_accuracy: 0.7049 - val_loss: 0.8538\n",
            "Epoch 10/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 324ms/step - accuracy: 0.7500 - loss: 1.0132 - val_accuracy: 0.6667 - val_loss: 0.6488\n",
            "Epoch 11/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m238s\u001b[0m 6s/step - accuracy: 0.7216 - loss: 0.8267 - val_accuracy: 0.6667 - val_loss: 0.9569\n",
            "Epoch 12/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 36ms/step - accuracy: 0.6562 - loss: 1.1635 - val_accuracy: 0.7222 - val_loss: 0.6450\n",
            "Epoch 13/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m259s\u001b[0m 6s/step - accuracy: 0.7644 - loss: 0.7451 - val_accuracy: 0.7326 - val_loss: 0.8131\n",
            "Epoch 14/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - accuracy: 0.6875 - loss: 0.7938 - val_accuracy: 0.6667 - val_loss: 0.7851\n",
            "Epoch 15/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m220s\u001b[0m 6s/step - accuracy: 0.7317 - loss: 0.7852 - val_accuracy: 0.6840 - val_loss: 0.8887\n",
            "Epoch 16/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 538ms/step - accuracy: 0.6875 - loss: 1.0545 - val_accuracy: 0.7222 - val_loss: 0.5940\n",
            "Epoch 17/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m228s\u001b[0m 5s/step - accuracy: 0.7600 - loss: 0.7761 - val_accuracy: 0.6979 - val_loss: 0.8598\n",
            "Epoch 18/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 17ms/step - accuracy: 0.8438 - loss: 0.5724 - val_accuracy: 0.4444 - val_loss: 1.6403\n",
            "Epoch 19/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m266s\u001b[0m 6s/step - accuracy: 0.7625 - loss: 0.7110 - val_accuracy: 0.7292 - val_loss: 0.7768\n",
            "Epoch 20/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.6875 - loss: 0.8580 - val_accuracy: 0.8333 - val_loss: 0.6989\n",
            "Epoch 21/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m253s\u001b[0m 6s/step - accuracy: 0.7774 - loss: 0.6798 - val_accuracy: 0.7083 - val_loss: 0.8214\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 2s/step - accuracy: 0.6745 - loss: 0.8938\n",
            "Validation Accuracy: 68.30%\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
            "Predicted Classes: [2 3 4 4 1 5 3 4 4 4 1 2 3 2 1 2 1 5 4 1 5 5 3 4 3 3 1 1 3 5 4 1]\n",
            "Actual Classes: [4 3 1 1 1 5 3 4 4 1 2 2 3 2 2 2 2 5 4 2 5 5 4 4 3 3 3 1 3 5 4 0]\n"
          ]
        }
      ]
    }
  ]
}